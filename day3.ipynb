{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiNDqKmUt4ZAf5TcrcoYsd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vatsa10/pytorch/blob/main/day3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network**"
      ],
      "metadata": {
        "id": "8LKr-VabxykM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "Pc4qmZ_Qx_1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0TVOVGoyAXO",
        "outputId": "1a5be3c4-3afa-47fb-dcc0-414b1e1c7148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.flatten(x)\n",
        "      logits = self.linear_relu_stack(x)\n",
        "      return logits"
      ],
      "metadata": {
        "id": "pePgD4a3yDO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCIq_4-wzahD",
        "outputId": "a7adced7-3f4e-4b4a-af59-8e120d72fb00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(1,28,28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0335-O8zmeV",
        "outputId": "a0ce10e5-e8b5-4528-eb9f-405ee97f1b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-aKPpulzu3O",
        "outputId": "d52c908a-8002-4813-cef5-6b2951647f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we are using nn.flatten to convert each 2D 28x28 image to an array of 784 pixel values\n",
        "\n",
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJGD_y6wz5IS",
        "outputId": "3b29d50d-aca6-45ce-e317-99a91165b108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.linear helps to apply linear transformation to the input using its stored weights and bias\n",
        "\n",
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sESA8zam0IW4",
        "outputId": "a6d34ff6-2670-4b5c-c9c1-84ce524b40e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.ReLU an activation between our linear layers to introduce non linearity,\n",
        "# to create a complex mapping between model's input and output, it helps in learning the NN the variety\n",
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4ihJO9G0Vi5",
        "outputId": "dcbaea9f-e8fb-4a54-c3ef-37108e0b08aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before ReLU: tensor([[ 0.0186,  0.0264,  0.2013, -0.4350,  0.3261,  0.2451, -0.5106, -0.2354,\n",
            "         -0.2482, -0.1536, -0.4635,  0.1739,  0.3640,  0.1085,  0.0801,  0.2293,\n",
            "          0.6897, -0.3096, -0.0235,  0.0882],\n",
            "        [ 0.3521, -0.0983, -0.0563, -0.2762,  0.3567,  0.1350, -0.6070, -0.1307,\n",
            "         -0.2070, -0.0341, -0.4157,  0.1981,  0.1114,  0.0379,  0.1457,  0.6690,\n",
            "          0.4446, -0.1523, -0.3109, -0.2374],\n",
            "        [ 0.5499, -0.1582,  0.0684, -0.7208,  0.1981, -0.0806, -0.4896, -0.0613,\n",
            "         -0.2780,  0.1717, -0.4365,  0.2430, -0.0728, -0.1148,  0.2786,  0.3404,\n",
            "          0.3951, -0.2482, -0.0797,  0.2373]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.0186, 0.0264, 0.2013, 0.0000, 0.3261, 0.2451, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.1739, 0.3640, 0.1085, 0.0801, 0.2293, 0.6897, 0.0000,\n",
            "         0.0000, 0.0882],\n",
            "        [0.3521, 0.0000, 0.0000, 0.0000, 0.3567, 0.1350, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.1981, 0.1114, 0.0379, 0.1457, 0.6690, 0.4446, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.5499, 0.0000, 0.0684, 0.0000, 0.1981, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.1717, 0.0000, 0.2430, 0.0000, 0.0000, 0.2786, 0.3404, 0.3951, 0.0000,\n",
            "         0.0000, 0.2373]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will be using nn.Sequential to pass the data through all the modules as same order it is defined in\n",
        "\n",
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20,10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ],
      "metadata": {
        "id": "uE1KQBzGzFBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Softmax is used as the last linear layer to return logits that are raw values from -infinity to infinity\n",
        "# which are then passed to softmax module then the logits are scaled to values [0,1]\n",
        "# representing the model's predicted probabilities for each class.\n",
        "\n",
        "softmax = nn.Softmax(dim=1)  #dim 1 because this paramater will indicate the dimension from the values must sum to 1\n",
        "pred_probab = softmax(logits)"
      ],
      "metadata": {
        "id": "glyLcx7tz0oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model Architecture; {model}\\n\\n\")\n",
        "for name, param in model.named_parameters():\n",
        "  print(f\"Layer: {name} | Size: {param.size()} | Values: {param[:2]} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVJNXuA70qjc",
        "outputId": "7f2f2d66-e8d4-4674-a743-9ca8ac93b74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Architecture; NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values: tensor([[-0.0078, -0.0263,  0.0009,  ...,  0.0233, -0.0163, -0.0221],\n",
            "        [-0.0130, -0.0234, -0.0248,  ..., -0.0161, -0.0304, -0.0115]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values: tensor([ 0.0352, -0.0288], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values: tensor([[-0.0028,  0.0108,  0.0024,  ...,  0.0245, -0.0184, -0.0200],\n",
            "        [-0.0017,  0.0342,  0.0275,  ...,  0.0159, -0.0137, -0.0309]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values: tensor([-0.0395,  0.0437], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values: tensor([[ 1.7865e-02, -7.2893e-03,  1.5548e-02,  ...,  3.9022e-02,\n",
            "         -1.4399e-02, -5.2957e-03],\n",
            "        [-2.5154e-02, -4.3947e-02, -3.8956e-03,  ...,  3.1846e-02,\n",
            "          6.2088e-05, -3.3583e-02]], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values: tensor([-0.0423,  0.0172], grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Automatic Differentiation with torch.autograd**"
      ],
      "metadata": {
        "id": "JVfthU4tciKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# back propagation is used to adjust the gradient of the loss function with repect to params\n",
        "\n",
        "import torch\n",
        "x = torch.ones(5)\n",
        "y = torch.zeros(3)\n",
        "w = torch.randn(5,3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x,w)+b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)"
      ],
      "metadata": {
        "id": "Jqm0wfSUch0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuyG41g3dmwz",
        "outputId": "9fa36f9d-1f58-441c-fece-5b7e2935a32c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x7fbc42d2e140>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fbc42969090>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to optimize weights of params in nn we will compute the derivatives of our loss function, we need sigma loss/weights and same for bias under some fixed value of x and y\n",
        "# to do this we call loss.backward() and then retrieve the values from w.grad and b.grad\n",
        "\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnaGlAsXdsI2",
        "outputId": "f15ff280-80ab-4dc0-d823-c88fc154b6e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3330, 0.1482, 0.2741],\n",
            "        [0.3330, 0.1482, 0.2741],\n",
            "        [0.3330, 0.1482, 0.2741],\n",
            "        [0.3330, 0.1482, 0.2741],\n",
            "        [0.3330, 0.1482, 0.2741]])\n",
            "tensor([0.3330, 0.1482, 0.2741])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when we train a model and just want to apply it to some input data, we dont want to forward the computation\n",
        "# we can stop tracking computation through newtwork by torch.no_grad()\n",
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiwT4knvefRC",
        "outputId": "f23bb1ae-2877-4b3a-962b-8fb257242d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# also we can do this by detach()\n",
        "z = torch.matmul(x, w)+b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5Bzi_EYfUcr",
        "outputId": "fe2c5488-5701-4d4c-a1c1-bf7cc0ad1b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "There are reasons you might want to disable gradient tracking:\n",
        "1). To mark some parameters in your neural network as frozen parameters.\n",
        "2). To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "avy7s5XZfbqW",
        "outputId": "69e0bedd-02f2-4101-94e4-c002af1f3e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThere are reasons you might want to disable gradient tracking:\\n1). To mark some parameters in your neural network as frozen parameters.\\n2). To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q47D4FN7fk2b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}